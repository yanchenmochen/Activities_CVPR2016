In this paper, we introduce a new hierarchical model for human action
recognition using body joint locations. This model is able to categorize complex actions 
performed in videos, as well as, to perform spatio-temporal annotations of the atomic
actions that compose the underlying complex action. That is, for each atomic
action, the model generates temporal atomic action annotations by
inferring its starting and ending times, as well as,
spatial annotations by inferring the human body parts that are involved
in each atomic action. Our model has three key properties:
(i) it can be trained with no spatial
supervision, as it is able to automatically discover active body parts
from temporal action annotations only;
(ii) it jointly learns representations for motion poselets and actionlets that encode the
visual variability of body parts and atomic actions, respectively;
(iii) it includes a mechanism for handling noisy body pose estimates which increases its 
robustness to
common pose estimation errors.
We experimentally evaluate the performance of our method using multiple
action recognition benchmarks. Our model consistently outperform baselines
and state-of-the-art action recognition methods. 

