We present a hierarchical model for 
human action recognition using body joint locations. By using a semisupervised approach to jointly 
learn dictionaries of motions poselets and actionlets, the model demonstrates to be very flexible 
and informative, to handle visual variations and to provide spatio-temporal annotations of 
relevant atomic actions and active body part configurations. In particular, the model demonstrates 
to be competitive with respect to state-of-the -art approaches for complex action recognition, 
while also proving highly valuable additional information. As future work, the model can be 
extended to handle multiple actor situations, to use contextual information such as relevant 
objects, and to identify novel complex actions not present in the training set.


