[IL addings]
 
Many benchmarks in action recognition are constructed using short videos of single human actions. As \cite{Lillo2014} and \cite{Wei2013} noticed, in a more realistic setup the  subjects are able to execute several actions at the same time, like waving a hand while calling by cellphone. This setups are known as \emph{complex}, \emph{composed}, or \emph{concurrent} actions according to the context. Using this kind of action configuration makes the inference difficult since we are dealing with a spatio/temporal multilabel setup: in a single video, multiple actions can appear, and they can be temporally or spatially composed. Models like \cite{Lillo2014}, \cite{Wei2013} addresses composed actions, both providing a new dataset using RBG-B sensors. On the other hand, a benchmark that issue single actions per video can benefit of a more complex model, identifying  temporal or spatial segments where the action actually occurs. While several models exists for single actions, only few models have been developed to tackle composed or concurrent actions.

In \cite{Lillo2014}, the authors provide the \emph{Composable Activities} dataset, where the atomic actions were fully annotated temporally and spatially: each human region (4 regions) have a different set of temporal annotations of actions, which make this dataset unique. However, this fact also makes the model created in top of this dataset not very useful for a more general setup, where only the temporal spanning of concurrent actions are annotated. In fact, databases of simple actions should be augmented in annotations,  which make the model not general for most action datasets. We improve the model in \cite{Lillo2014} by treating the spatial assignments of actions as latent variables. 