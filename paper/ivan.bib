@article{Cheron2015,
abstract = {This work targets human action recognition in video. While recent methods typically represent actions by statistics of local video features, here we argue for the importance of structural information derived from human poses. To this end we propose a new Pose-based Convolutional Neural Network descriptor (P-CNN) for action recognition. The descriptor aggregates motion and appearance information along tracks of human body parts. We investigate different schemes of temporal aggregation and experiment with P-CNN features obtained both for automatically estimated and manually annotated human poses. We evaluate our method on the recent and challenging JHMDB and MPII Cooking datasets. For both datasets our method shows consistent improvement over the state of the art.},
archivePrefix = {arXiv},
arxivId = {1506.03607},
author = {Ch{\'{e}}ron, Guilhem and Laptev, Ivan and Schmid, Cordelia},
eprint = {1506.03607},
file = {::},
journal = {ICCV},
title = {{P-CNN: Pose-based CNN Features for Action Recognition}},
url = {http://arxiv.org/abs/1506.03607},
year = {2015}
}
@inproceedings{YongDu2015,
author = {{Yong Du} and Wang, Wei and Wang, Liang},
booktitle = {CVPR},
doi = {10.1109/CVPR.2015.7298714},
file = {::},
isbn = {978-1-4673-6964-0},
month = {jun},
pages = {1110--1118},
publisher = {IEEE},
title = {{Hierarchical recurrent neural network for skeleton based action recognition}},
url = {http://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2015/app/1A{\_}121.pdf http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7298714},
year = {2015}
}
@inproceedings{Hu2015,
author = {Hu, Jian-Fang and Zheng, Wei-Shi and Lai, Jianhuang and Zhang, Jianguo},
booktitle = {CVPR},
doi = {10.1109/CVPR.2015.7299172},
file = {::},
isbn = {978-1-4673-6964-0},
month = {jun},
pages = {5344--5352},
publisher = {IEEE},
title = {{Jointly learning heterogeneous features for RGB-D activity recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7299172},
year = {2015}
}
@inproceedings{Hu2014,
abstract = {We present a novel latent discriminative model for human activity recognition. Unlike the approaches that require conditional independence assumptions, our model is very flexible in encoding the full connectivity among observations, latent states, and activity states. The model is able to capture richer class of contextual information in both state-state and observation-state pairs. Although loops are present in the model, we can consider the graphical model as a linear-chain structure, where the exact inference is tractable. Thereby the model is very efficient in both inference and learning. The parameters of the graphical model are learned with the Structured-Support Vector Machine (Structured-SVM). A data-driven approach is used to initialize the latent variables, thereby no hand labeling for the latent states is required. Experimental results on the CAD-120 benchmark dataset show that our model outperforms the state-of-the-art approach by over 5{\%} in both precision and recall, while our model is more efficient in computation.},
author = {Hu, Ninghang and Englebienne, Gwenn and Lou, Zhongyu and Krose, Ben},
booktitle = {ICRA},
doi = {10.1109/ICRA.2014.6906983},
file = {::},
isbn = {978-1-4799-3685-4},
month = {may},
pages = {1048--1053},
publisher = {IEEE},
title = {{Learning latent structure for activity recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6906983},
year = {2014}
}
@inproceedings{Kong2015,
abstract = {RGB-D Camera에 기반한 action recognition 방법 소개},
author = {Kong, Yu and Fu, Yun},
booktitle = {CVPR},
doi = {10.1109/CVPR.2015.7298708},
file = {::},
isbn = {978-1-4673-6964-0},
month = {jun},
pages = {1054--1062},
publisher = {IEEE},
title = {{Bilinear heterogeneous information machine for RGB-D action recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7298708},
year = {2015}
}
@inproceedings{Nie2015,
author = {Nie, Bruce Xiaohan and Xiong, Caiming and Zhu, Song-chun},
booktitle = {CVPR},
doi = {10.1109/CVPR.2015.7298734},
file = {::},
isbn = {978-1-4673-6964-0},
month = {jun},
pages = {1293--1301},
publisher = {IEEE},
title = {{Joint action recognition and pose estimation from video}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7298734},
year = {2015}
}
@inproceedings{Rohrbach2012,
abstract = {While activity recognition is a current focus of research the challenging problem of fine-grained activity recognition is largely overlooked. We thus propose a novel database of 65 cooking activities, continuously recorded in a realistic setting. Activities are distinguished by fine-grained body motions that have low inter-class variability and high intra-class variability due to diverse subjects and ingredients. We benchmark two approaches on our dataset, one based on articulated pose tracks and the second using holistic video features. While the holistic approach outperforms the pose-based approach, our evaluation suggests that fine-grained activities are more difficult to detect and the body model can help in those cases. Providing high-resolution videos as well as an intermediate pose representation we hope to foster research in fine-grained activity recognition.},
author = {Rohrbach, Marcus and Amin, Sikandar and Andriluka, Mykhaylo and Schiele, Bernt},
booktitle = {CVPR},
doi = {10.1109/CVPR.2012.6247801},
file = {::},
isbn = {978-1-4673-1228-8},
issn = {10636919},
month = {jun},
number = {June},
pages = {1194--1201},
publisher = {IEEE},
title = {{A database for fine grained activity detection of cooking activities}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6247801},
year = {2012}
}
@article{Tao2015,
author = {Tao, Lingling and Vidal, Rene},
file = {:Users/jniebles/Library/Application Support/Mendeley Desktop/Downloaded/Tao - 2015 - Moving Poselets A Discriminative and Interpretable Skeletal Motion Representation for Action Recognition.pdf:pdf},
journal = {ChaLearn Looking at People Workshop},
title = {{Moving Poselets : A Discriminative and Interpretable Skeletal Motion Representation for Action Recognition}},
year = {2015}
}
@inproceedings{Vemulapalli2014,
author = {Vemulapalli, Raviteja and Arrate, Felipe and Chellappa, Rama},
booktitle = {CVPR},
doi = {10.1109/CVPR.2014.82},
file = {::},
isbn = {978-1-4799-5118-5},
issn = {10636919},
month = {jun},
pages = {588--595},
publisher = {IEEE},
title = {{Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909476},
year = {2014}
}
@inproceedings{Wang2014,
author = {Wang, Limin and Qiao, Yu and Tang, Xiaoou},
booktitle = {ECCV},
doi = {10.1007/978-3-319-10602-1{\_}37},
file = {::},
keywords = {action detection,dynamic-poselet,sequential skeleton model},
pages = {565--580},
title = {{Video Action Detection with Relational Dynamic-Poselets}},
url = {http://link.springer.com/10.1007/978-3-319-10602-1{\_}37},
year = {2014}
}
@article{Yeung2015,
abstract = {Every moment counts in action recognition. A comprehensive understanding of human activity in video requires labeling every frame according to the actions occurring, placing multiple labels densely over a video sequence. To study this problem we extend the existing THUMOS dataset and introduce MultiTHUMOS, a new dataset of dense labels over unconstrained internet videos. Modeling multiple, dense labels benefits from temporal relations within and across classes. We define a novel variant of long short-term memory (LSTM) deep networks for modeling these temporal relations via multiple input and output connections. We show that this model improves action labeling accuracy and further enables deeper understanding tasks ranging from structured retrieval to action prediction.},
archivePrefix = {arXiv},
arxivId = {1507.05738},
author = {Yeung, Serena and Russakovsky, Olga and Jin, Ning and Andriluka, Mykhaylo and Mori, Greg and Fei-Fei, Li},
eprint = {1507.05738},
file = {::},
title = {{Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos}},
url = {http://arxiv.org/abs/1507.05738},
year = {2015}
}
@inproceedings{Zanfir2013,
abstract = {Human action recognition under low observational latency is receiving a growing interest in computer vision due to rapidly developing technologies in human-robot interaction, computer gaming and surveillance. In this paper we propose a fast, simple, yet powerful non-parametric Moving Pose (MP) framework for low-latency human action and activity recognition. Central to our methodology is a moving pose descriptor that considers both pose information as well as differential quantities (speed and acceleration) of the human body joints within a short time window around the current frame. The proposed descriptor is used in conjunction with a modified kNN classifier that considers both the temporal location of a particular frame within the action sequence as well as the discrimination power of its moving pose descriptor compared to other frames in the training set. The resulting method is non-parametric and enables low-latency recognition, one-shot learning, and action detection in difficult unsegmented sequences. Moreover, the framework is real-time, scalable, and outperforms more sophisticated approaches on challenging benchmarks like MSR-Action3D or MSR-DailyActivities3D. View full abstract},
author = {Zanfir, Mihai and Leordeanu, Marius and Sminchisescu, Cristian},
booktitle = {ICCV},
doi = {10.1109/ICCV.2013.342},
file = {::},
isbn = {978-1-4799-2840-8},
issn = {1550-5499},
month = {dec},
pages = {2752--2759},
publisher = {IEEE},
title = {{The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6751453$\backslash$npapers3://publication/doi/10.1109/ICCV.2013.342 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6751453},
year = {2013}
}

@article{Raptis2012,
abstract = {We describe a mid-level approach for action recognition. From an input video, we extract salient spatio-temporal structures by forming clusters of trajectories that serve as candidates for the parts of an action. The assembly of these clusters into an action class is governed by a graphical model that incorporates appearance and motion constraints for the individual parts and pairwise constraints for the spatio-temporal dependencies among them. During training, we estimate the model parameters discriminatively. During classification, we efficiently match the model to a video using discrete optimization. We validate the model's classification ability in standard benchmark datasets and illustrate its potential to support a fine-grained analysis that not only gives a label to a video, but also identifies and localizes its constituent parts.},
author = {Raptis, Michalis and Kokkinos, Iasonas and Soatto, Stefano},
doi = {10.1109/CVPR.2012.6247807},
file = {:home/ivan/Documents/Mendeley Desktop/Raptis, Kokkinos, Soatto - 2012 - Discovering discriminative action parts from mid-level video representations - CVPR.pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
journal = {CVPR},
pages = {1242--1249},
title = {{Discovering discriminative action parts from mid-level video representations}},
year = {2012}
}
@article{Jhuang2013,
abstract = {Although action recognition in videos is widely studied, current methods often fail on real-world datasets. Many recent approaches improve accuracy and robustness to cope with challenging video sequences, but it is often unclear what affects the results most. This paper attempts to provide insights based on a systematic performance evaluation using thoroughly-annotated data of human actions. We annotate human Joints for the HMDB dataset (J-HMDB). This annotation can be used to derive ground truth optical flow and segmentation. We evaluate current methods using this dataset and systematically replace the output of various algorithms with ground truth. This enables us to discover what is important - for example, should we work on improving flow algorithms, estimating human bounding boxes, or enabling pose estimation? In summary, we find that high-level pose features greatly outperform low/mid level features, in particular, pose over time is critical, but current pose estimation algorithms are not yet reliable enough to provide this information. We also find that the accuracy of a top-performing action recognition framework can be greatly increased by refining the underlying low/mid level features, this suggests it is important to improve optical flow and human detection algorithms. Our analysis and J-HMDB dataset should facilitate a deeper understanding of action recognition algorithms. View full abstract},
author = {Jhuang, Hueihan and Gall, Juergen and Zuffi, Silvia and Schmid, Cordelia and Black, Michael J.},
doi = {10.1109/ICCV.2013.396},
file = {:home/ivan/Documents/Mendeley Desktop/Jhuang et al. - 2013 - Towards understanding action recognition - Proceedings of the IEEE International Conference on Computer Vision.pdf:pdf},
isbn = {9781479928392},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
keywords = {JHMDB,action recognition,annotation,dataset,optical flow estimation,pose estimation},
pages = {3192--3199},
title = {{Towards understanding action recognition}},
year = {2013}
}
