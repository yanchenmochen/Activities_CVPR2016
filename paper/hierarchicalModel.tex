%\begin{comment}
%\subsection{Latent spatial actions for hierarchical action detection}
% %
%[CVPR2014 MODEL SUMMARY] In order to achieve or objectives, we build our model on top of the hierarchical model presented in \cite{Lillo2014}.
%The authors model an activity using three hierarchical levels: poses, atomic actions, and activities, using features extracted from 3D skeleton joints, splitting the human body into $R=4$ fixed spatial regions, corresponding to overlapping joints for arms and legs. It's a good sttr since the model considers energy potentials for poses as linear classifiers applied to frame features, selecting the pose classifier via a latent pose label $z_t \in \{1,\dots,K\}$. The pose modeling resembles Poselets [REF], In the mid-level of atomic actions, the model use as descriptor the BoW (histogram) representation of labels $Z$ for frames belonging to each action, in order to learn $A$ atomic action linear classifiers. The action label of a frame is denoted as $v_t \in \{1,\dots,A\}$. Finally, in the higher level of activities, the model use the BoW representation of actions labels $V$ of the video, to learn a multiclass linear classifier. Adding all energy terms, using $R=1$ for simplicity, and including temporal transition terms, the energy equation for video $D$ in \cite{Lillo2014} is 
%%
%\begin{eqnarray}
%\label{eq:energy2014}
%\begin{split}
%E(D) =  {\alpha_y}^\top h(V) + \sum_{a=1}^A {\beta_a}^\top h_a(Z) + \sum_{t=1}^T {w_{z_t}}^\top x_t  \\+ {\gamma}^\top h(V^{-1},V) + {\eta}^\top h(Z^{-1},Z)
%\end{split}
%\end{eqnarray}
%%
%where $h(V)$ is the histogram of atomic action labels, $h_a(Z)$ the histogram of pose labels at those frames labeled with action $a$, and $h(V^{-1},V)$ and $h(Z^{-1},Z)$ represents histograms for action and pose transitions respectively. The main idea is that the energy score for the video using the correct activity must be higher than every other activity. As during inference the labels $Z$ and $V$ are unknown, they must be inferred along with the activity label. Thus, an interesting property of this model is the per-frame nature of the energy function. The authors in \cite{Lillo2014} test their model in their own Composable Activities dataset, which include rich action labels annotated independently for four human regions (arms and legs). To date, this is the only action dataset with this level of spatial annotations. Our work take this model as an starting point to include flexibility... [CONTINUE]
%\end{comment}

\subsection{Hierarchical activity model}

Suppose we have a video $D$ with $T$ frames, each frame described by a feature vector $x_t$. Assume we have available $K$ classifiers$\{w_k\}_{k=1}^K$ over the frame descriptors, such that each frame descriptor can be associated to a single classifier. If we choose the maximum response for every frame, encoded as $z_t = \argmax_k\{w_k^\top x_t\}$,  we can build a BoW representation to feed linear action classifiers $\beta$, computing the histogram $h(Z)$ of $Z = \{z_1,z_2,\dots,z_T\}$ and using these histograms as a feature vector for the complete video to recognize single actions. Imagine now that we would like to use the scores of the maximum responses, $w_{z_t}^\top x_t$ as a potential to help discriminating between videos that present reliable poses from videos that does not. We can build a joint energy function, combining the action classifier score and the aggregated frame classifier scores, as
\begin{equation}
\label{eq:2-levels}
\begin{split}
E(D) &= \beta_{a}^\top h(Z) + \sum_{t=1}^T w_{z_t}^\top x_t \\ & = \sum_{t=1}^T\sum_{k=1}^K\left(\beta_{a,k} + w_k^\top x_t \right)\delta(z_t=k)
\end{split}
\end{equation}
What is interesting of Eq. (\ref{eq:2-levels}) is that it every term in the sum is tied for the value of $z_t$, creating a model such that all its components depends of the labeling $Z$. We can expand the previous model to more levels using the same philosophy. In fact, for a new level, we could create a new indicator $v_t$ for every frame that indicates the election of which classifier $\beta$ will be used (the same as $z_t$ indicates which classifier of $w$). If we name $w$ as \emph{pose classifiers}, and $\beta$ as \emph{action classifiers}, we can create a hierarchical model where multiple poses and actions can be present in a single video. Supposing we have $A$ actions; the energy for a three-level hierarchy could be, for an \emph{activity} $l$,
\begin{equation}
E(D) =\alpha_l^\top h(V) + \sum_{a=1}^A \beta_{a}^\top h^a(Z,V) + \sum_{t=1}^T w_{z_t}^\top x_t 
\end{equation}
where $h^a(Z,V)$ refers to the BoW representation of $Z$ for those frames labeled as action $v_t = a$.

[NEW MODEL]
%We propose three improvements to the model to make it more practical and flexible: firstly, to relax the need of action annotations for each human region, we propose to use latent variables representing the assignment of actions to human body regions; secondly, to overcome the multimodal representations of actions, we model each action as a group of linear classifiers instead of using a single classifier per action; and last, to handle the noisy or non-informative poses (NI), we propose a \emph{garbage collector} approach where the model itself identifies a threshold for the scores of pose classifiers of every human region. We describe each contribution in the following paragraphs.

Recent work in action recognition \cite{Cheron2015,Tao2015, Wang2011,Jhuang2013} shows a resurgence of describing human actions as a collection of dynamic spatial parts that resembles Poselets. In line with these research, we split the human body into $R$ semantic regions. As modeling actions using the whole body is hard, separating the body into groups of limbs helps in recognition of actions, specially for complex datasets \cite{Tao2015}. Our wiew is that while poses are in general well defined in most research, little effort has been made to mine actions from videos, in terms of detecting the temporal spanning (action detection) and action localization. In addition to the fact that most action datasets are only single actions, there is a lack of research in the general setup where actions are combined in the same video. Nevertheless, a few works have noticed that humans usually performs complex action in real life \cite{Wei2013, Lillo2014}, providing their own datasets based in RGB-D cameras. In our work, we aim to group both worlds of single and composed actions in a single hierarchical model of three semantic levels, and using human body regions to improve the representativeness. 

During training, we assume there is temporal annotations of actions. As we want our model to perform action localization, we model the action assignments $V_r$ in each region as latent variables during training, allowing the model to infer which human part execute the action without needing this kind of annotations in the training set, including a model for the initialization of action labels. In this way, we advance from a simple detection problem to infer also \emph{how} the subject executes the action, important in surveillance applications, health monitoring, between others. We also expand the modeling of recurrent patterns of poses to construct a general model for shared actions, aiming to handle multimodal information, which is produced by actions with the same label but with different execution patterns, or by changes in representation of actions such as varying camera view. We handle this problem by augmenting the number of action classifiers, where each original action acts as a parent node of several non-overlapping child actions. Finally, as we are using local information for poses, some frames could be noisy or representing an uncommon pose, not useful to build the pose models. We attack this issue by adding a garbage collector for poses, where only the most-informative poses are used by pose classifiers during learning. We describe these contributions in the following paragraphs.

\paragraph{[EDIT] Latent assignments of actions to human regions}

Knowing the parts of the body involved in the actions is highly appealing. Suppose we have $M$ videos, each video annotated with $Q_m$ action intervals. Each action interval can be associated with any number of regions, from $1$ to all $R$ regions. For example,  a \emph{waving hand} action could be associated only with \emph{right\_arm}, while the action \emph{jogging} could be associated with the whole body. We want to learn the associations of actions and human parts for training videos, and we build these associations using latent variables. The main problem to solve is to how to get a proper initialization for actions, since there is a very high chance to get sticked in a local minimum far away of the optimum, producing bad results.

Our first contribution is a method to get a proper initialization of fine-grained spatial action labels, knowing only the time span of the actions. Using the known action intervals,  we formulate the problem of action to region assignment as an optimization problem, constrained using structural information: the actions intervals must not overlap in the same region, and all the action intervals must be present at least in one region. We formulate this labeling problem as a binary Integer Linear Programming (ILP) problem. We define as $v_{r,q}^m=1$ when the action interval $q \in \{1,\dots,Q_m\}$ appears in region $r$ in the video $m$, and $v_{r,q}^m=0$ otherwise. We assume we have pose labels $z_{t,r}$ in each frame, independent for each region, learned via clustering the poses for all frames in all videos. For an action interval $q$, we use as descriptor the histogram of pose labels for each region in the action interval, defined for the video $m$ as $h_{r,q}^m$ . We can solve the problem of finding the correspondence between action intervals and regions in a formulation similar to $k$-means, using the structure of the problem as constraints in the labels, and using $\chi^2$ distance between the action interval descriptors and the cluster centers: 
\begin{equation}
\begin{split}
P1) \quad \min_{v,\mu} &\sum_{m=1}^M  \sum_{r=1}^R \sum_{q=1}^{Q_m}  v_{r,q}^m d( h_{r,q}^m - \mu_{a_q}^r) -\frac{1}{\lambda} v_{r,q}^m\\ 
 \text{s. to} 
\quad 
& \sum_{r=1}^R v_{r,q}^m \ge 1\text{, }\forall q\text{, }\forall m \\ 
%& \sum_{q=1}^{Q_m} v_{r,q}^m \le t_m \\ 
& v_{r,q_1}^m + v_{r,q_2}^m \le 1 \text{ if } q_1\cap q_2 \neq \emptyset \text{, }\forall r\text{, }\forall m\\  
& v_{r,q}^m \in \{0,1\}\text{, }\forall q\text{, }\forall{r}\text{, }\forall m
\end{split}
\end{equation}
with
\begin{equation}
d( h_{r,q}^m - \mu_{a_q}^r) = \sum_{k=1}^K (h_{r,q}^m[k] - \mu_{a_q}^r[k])^2/(h_{r,q}^m[k] +\mu_{a_q}^r[k]).
\end{equation}

$\mu_{a_q}^r$ are computed as the mean of the descriptors with the same action label within the same region. We solve $P1$ iteratively as $k$-means,  finding the cluster centers for each region $r$, $\mu_{a}^r$ using the labels $v_{r,q}^m$, and then finding the best labeling given the cluster centers, solving an ILP problem. Note that the first term of the objective function is similar to a $k$-means model, while the second term resembles the objective function of \emph{self-paced} learning as in \cite{Kumar2010}, fostering to balance between assigning a single region to every action, towards assigning all possible regions to the action intervals when possible.  

[IL: INCLUDE  FIGURE TO SHOW P1 GRAPHICALLY]

We describe the further changes in the hierarchical model of \cite{Lillo2014} in the learning and inference sections.
  \paragraph{[EDIT] Representing semantic actions with multiple atomic sequences}.


As the poses and atomic actions in \cite{Lillo2014} model are shared, a single classifier is generally not enough to model multimodal representations, that occur usually in  complex videos. We modify the original hierarchical model of \cite{Lillo2014} to include multiple linear classifiers per action. We create two new concepts: \textbf{semantic actions}, that refer to actions \emph{names} that compose an activity; and \textbf{atomic sequences}, that refers to the sequence of poses that conform an action. Several atomic sequences can be associated to a single semantic action, creating disjoint sets of atomic sequences, each set associated to a single semantic action.  The main idea is that the action annotations in the datasets are associated to semantic actions, whereas for each semantic action we learn several atomic sequence classifiers. With this formulation, we can handle the multimodal nature of semantic actions, covering the changes in motion, poses , or even changes in meaning of the action according to the context (e.g. the semantic action ``open'' can be associated to opening a can, opening a door, etc.). 

Inspired by \cite{Raptis2012}, we first use the \emph{Cattell's Scree test} for finding a suitable number of atomic sequence for every semantic action. Using the semantic action labels, we compute a descriptor for every interval using normalized histograms of pose labels. Then, for a particular semantic action $u$, we compute the the eigenvalues $\lambda_u$ of the affinity matrix of the semantic action descriptors, using $\chi^2$ distance. For each semantic action $u \in \{1,\dots,U\}$ we find the number of atomic sequences $G_u$ as $G_u = \argmin_i \lambda_{i+1}^2 / (\sum_{j=1}^i \lambda_j) + c\cdot i$, with $c=2\cdot 10^{-3}$. Finally, we cluster the descriptors corresponding to each semantic action using k-means, using a different number of clusters for each semantic action $u$ according to $G_u$. This approach generates non-overlapping atomic sequences, each associated to a single semantic action.

To transfer the new labels to the model, we define $u(v)$ as the function that given the atomic sequence label $v$, returns the corresponding semantic action label $u$. The energy for the activity level is then
\begin{equation}
E_{\text{activity}} =  \sum_{u=1}^U\sum_{t=1}^T \alpha_{y,u}\delta(u(v_t)=u)
\end{equation}  

For the action and pose labels the model remains unchanged. Using the new atomic sequences allows a richer representation for actions, while in he activity level, several atomic sequences will map to a single semantic action. This behavior resembles a max-pooling operation, where we will choose at inference the atomic sequences that best describe the performed actions in the video, keeping the semantics of the original labels. 

\paragraph{Towards a better representation of poses: adding a garbage collector}

The model in \cite{Lillo2014} uses all poses to feed action classifiers. Out intuition is that only a subset of poses in each video are really discriminative or informative for the actions performed, while there is plenty of poses that corresponds to noisy or non-informative ones. [EXPAND] Our intuition is that low-scored frames in terms of poses (i.e. a low value of $w_{z_t}^\top x_t$ in Eq. (\ref{eq:energy2014})) make the same contribution as high-scored poses in higher levels of the model, while degrading the pose classifiers at the same time since low-scored poses are likely to be related to non-informative frames. We propose to include a new pose, to explicitly handling those low-scored frames, keeping them apart for the pose classifiers $w$, but still adding a fixed score to the energy function to avoid normalization issues and to help in the specialization of pose classifiers. We call this change in the model a \emph{garbage collector} since it handles all low-scores frames and group them having a fixed energy score $\theta$. In practice, we use a special pose entry $K+1$ to identify the non-informative poses. The equation representing the energy for pose level is
%
\begin{equation} \label{Eq_poseEnergy}
E_{\text{poses}}  = \sum_{t=1}^T \left[  {w_{z_t}}^\top x_{t}\delta(z_{t} \le  K) + \theta 
\delta(z_{t}=K+1)\right] 
\end{equation}
where $\delta(\ell) = 1$ if $\ell$ is true and $\delta(\ell) = 0$ if
$\ell$ is false. The action level also change its energy:
\begin{equation}
\begin{split}
 \label{Eq_actionEnergy}
E_{\text{actions}} =  \sum_{t=1}^T \sum_{a=1}^A \sum_{k=1}^{K+1}  \beta_{a,k} \delta(z_t = k) \delta(v_t = a).
\end{split}
\end{equation}

\begin{comment}
Integrating all contribution detailed in previous sections, the model is written as:
Energy function:
\begin{equation}
E = E_{\text{activity}} + E_{\text{action}} + E_{\text{pose}}
  + E_{\text{action transition}} + E_{\text{pose transition}}.
\end{equation}

\begin{equation}
E_{\text{poses}}  = \sum_{t=1}^T \left[  {w_{z_t}}^\top x_{t}\delta(z_{t} \le  K) + \theta 
\delta(z_{t}=K+1)\right] 
\end{equation}

\begin{equation}
E_{\text{actions}} =  \sum_{t=1}^T \sum_{a=1}^A \sum_{k=1}^{K+1}  \beta_{a,k} \delta(z_t = k) \delta(v_t = a).
\end{equation}

\begin{equation}
h_g^{r}(U) = \sum_{t} \delta_{u_{t,r}}^g
\end{equation}

So the energy in the activity level is
\begin{equation}
E_{\text{activity}} = \sum_{r} {\alpha^r_{y}}^\top h^{r}(U) = \sum_{r,g,t}  \alpha^r_{y,g} \delta_{u_{t,r}}^g
\end{equation}

\begin{equation}
E_{\text{action transition}} = \sum_{r,a,a'}  \gamma^r_{a',a} \sum_{t} \delta_{v_{t-1,r}}^{a'}\delta_{v_{t,r}}^a 
\end{equation}

\begin{equation}
E_{\text{pose transition}} =\sum_{r,k,k'}  \eta^r_{k',k}\sum_{t}\delta_{z_{t-1,r}}^{k'}\delta_{z_{t,r}}^{k}
\end{equation}
\end{comment}


