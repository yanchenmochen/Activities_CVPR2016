Human activity recognition is a key technology for the development of many
important computer vision applications, such as
surveillance, human-computer interaction, and video annotation, among others.
Consequently, it has received wide attention in the computer vision community
\cite{Aggarwal2011,vishwakarma2013survey,weinland2011survey}
with a strong focus on recognition of atomic actions using RGB videos.
Recently, the emergence of capable and affordable RGB-D cameras
has opened a new attractive
scenario to recognize complex human activities. As a major
advantage, RGB-D data facilitates the
segmentation of the human body, as well as, the identification of relevant
interest points, such as body joint positions\cite{Shotton2011}, which are much more
difficult to identify directly from color images only.
In this paper, we build upon this recent success and present an approach
for human activity recognition that operates on body poses estimated from
color and depth images.

In general, a human activity is composed of a set of atomic actions that can be
executed sequentially or simultaneously by different body regions.
These compositions can occur spatially and/or temporally, and they
can involve
interactions with the environment, other people, or specific objects.
For instance, people can text while walking, or wave one
hand while holding a phone to their ear with the other
(Fig.~\ref{fig:frontfigure}).
It is interesting to note that different
compositional arrangements of poses and atomic actions can yield different
semantics at a higher level.
Therefore, it is important for activity recognition systems
to be aware of such compositional differences when discriminating
activities at a high level.

In this work, we present a new approach for recognizing human activities using RGB-D data. In 
particular, we focus on activities that can be characterized by the body motions of a single actor. 
A key aspect of our method is a novel hierarchical compositional model that operates at three levels 
of abstraction. At the bottom level, our model learns a dictionary of representative body pose 
primitives. At the mid-level, these body poses are combined to compose atomic actions, such as, 
walking or reading. Finally, at the top-level, atomic actions are combined to compose complex human 
activities, such as walking while talking on the phone and waving a hand. Additionally, our model 
also considers spatial and temporal relations among poses and atomic actions.

The use of intermediate abstraction levels that have a direct semantic interpretation, such as body 
poses or atomic actions, provides several advantages. At training time, it can take advantage of 
labeled data that can be available at intermediate abstraction levels. At test time, it enables the 
model to automatically identify useful information, such as the temporal span of atomic actions or 
the regons of the body executing the action. The inference of semantic information at the 
intermediate levels makes a notable difference with respect to blind compositional models based on 
deep learning techniques \cite{Bengio:09}. Additionally, the use of a compositional model can 
naturally handle scenarios of partial occlusions and pose estimation failures by inferring an 
appropriate spatial composition of visible and relevant body regions while dismissing the occluded or 
irrelevant ones.

We formulate learning as an energy minimization problem, where structural hierarchical relations are 
modeled by sub-energy terms that constraint compositions among poses and actions, as well as, their 
spatial and temporal relations. Additionally, the energy function is complemented by regularization 
terms that foster the inference of a dictionary of body pose primitives that shares discriminative 
poses among action classes. This enables the model to use small dictionary sizes, to reduce 
over-fitting problems, and to improve computational efficiency.

We presented a preliminary version of this framework in \cite{Lillo2014}.
The extended version presented here
introduces three main modifications to the original model as well as new extensive experimental 
validations. First,
while the work in \cite{Lillo2014} is based on geometric features extracted from body joint 
positions, we now explore the addition of feature descriptors based on appearance and motion cues 
extracted from RGB images.
Second, while the work in \cite{Lillo2014} needs to associate all body poses in all video 
frames to one of the available atomic 
actions, we now include a new mechanism that identifies and discards non-informative frames. Third, 
while the work in \cite{Lillo2014} uses a quadratic regularizer to constraint model parameters, we 
now explore the use of a new regularizer that fosters group sparsity among the coefficients of the 
action 
classifiers. These extensions result in improved recognition performance
and increased computational efficiency over the preliminary version of our model. We validate these 
considerations conducting extensive experimental validations and comparing the performance of our 
approach against different baselines and related works.

The rest of the paper is organized as follows.
Sec.~\ref{sec:related_work} reviews relevant prior work.
Sec.~\ref{sec:model} introduces the details of our model and discusses learning
and inference algorithms.
Sec.~\ref{sec:experiments} presents empirical evaluations of
our method using benchmark datasets.
Finally, Sec.~\ref{sec:conclusions} concludes the paper.


%Human activity recognition by automatic selection of relevant body parts.

%Motivation: annotation may be innacurate, containing frames with
%unrelated actions/motions.

%, as well as, new activities characterized by novel compositions of % atomic
%actions and poses.




