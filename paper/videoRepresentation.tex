\subsection{Video Representation} \label{subsec:videorepresentation}

[EXPLAIN BETTER]
Our model is based on skeleton information encoded in joint annotations.  We use the same geometric descriptor as in \cite{Lillo2014}, using angles between segments connecting two joints, and angles between these segments and a plane formed by three joints. In addition to geometry, other authors \cite{Zanfir2013,Tao2015,Wang2014} have noticed that including local motion information is beneficial to the categorization of videos. Moreover, in \cite{zhu2013fusing} the authors create a fused descriptor using spatio-temporal descriptors and joint descriptors, showing that combined perform better than separated. With this is mind, we augment the original geometric descriptor with motion information: when there is not RGB data, we use the displacement of vectors (velocity) as a motion descriptor. If RGB video is available, we use the HOF descriptor extracted from the trajectory of the joint in a small temporal window.

For the geometric descriptor, we use 6 segments per human action (see Fig. XXXX). The descriptor is composed by the angles between segments (15 angles), and the angles between a plane formed by three segments and the non-coplanar segments (3 angles). For motion descriptor, we use either the 3D velocity of every joint in each region (18 dimensions), or the HOF descriptor of the joint trajectories concatenated and transformed using PCA (20 dimensions).