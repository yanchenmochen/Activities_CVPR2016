In this section, we present experimental results to study the performance
of our model in the task of complex action recognition. Our experimental
validation focuses on measuring two aspects of our model.
First, we measure action classification accuracy on
several action recognition benchmarks.
Second, we validate the performance of our model at the
task of spatio-temporal atomic action
annotation of human body regions that are involved in performing the complex
action.

%This section described the experimental results in heterogeneous datasets. We
%first describes each dataset to specify the experimental setup, whith a brief
%explanation of why we choose each dataset to evaluate our model. Later, we
%focus in the strenght of our model, in producing rich annotations in addition
%to the semantic global labels. 

%\subsection{Experimental Setup}
In the following experiments, we evaluate our method on four action recognition
benchmarks:
the MSR-Action3D dataset \cite{WanLi2010},
Concurrent Actions dataset \cite{Wei2013},
Composable Activities Dataset \cite{Lillo2014}, and sub-JHMDB
\cite{Jhuang2013}.


\subsection{Classification of Simple and Isolated Actions}

As a first experiment,
we evaluate the performance of our model on the task of simple and
isolated human action recognition in the  MSR-Action3D dataset
\cite{WanLi2010}.
Note that although our model is tailored at recognition of complex human
actions, this experiment verifies the performance of our model in the
simpler scenario of isolated atomic action classification.

The MSR-Action3D dataset provides depth videos and estimated body poses
for isolated actors in pre-trimmed videos performing actions from 20
categories. We use 557 videos in the dataset in a similar setup to
\cite{Wang2012}, where videos from subjects 1, 3, 5, 7, 9 are used for
training and the rest for testing.

We measure action classification accuracy and report the results obtained
by our model and other competing methods in Table \ref{tab:msr3d}.
We note that our model achieves comparable performance with respect
to the state-of-the-art methods for simple action recognition.

\begin{table}[t]
\footnotesize
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Algorithm} & \textbf{Accuracy}\\
\hline
Our model &  93.0\% \\
%Ours, GEO+VEL, NI &  93.0\% \\
%Ours, GEO+VEL  & 91.2\% \\
\hline
L. Tao \etal \cite{Tao2015} & 93.6\% \\
C. Wang \etal \cite{Wang2013} &    90.2\% \\
Vemulapalli \etal \cite{Vemulapalli2014} & 89.5\% \\
%Lillo et al. \cite{Lillo2014} & 89.5\%\\
\hline
\end{tabular}
\caption{\footnotesize
Action classification performances in the MSR-Action3D dataset. }
\vspace{-4mm}
\label{tab:msr3d}
\end{table}

%\paragraph{MSR-Action3D} Setup: subjects 1,3,5,7,9 for training, rest for
%testing, using all 20 action categories. This dataset is more as a proof of
%concept, that our model achieves near state-of-the-art accuracy in a standard
%dataset.
%In fact, omitting Tao el at. ICCV 2015 paper, we would achieve the
%best accuracy. BUT, they do not provide the rich annotations for testing data
%as our model. Also, we use the same initialization method to automatically
%annotate the actions in the dataset: the initialization method is integrated
%with the model and is independent of the dataset.

%\paragraph{MSR-Action3D} A very popular skeleton + Depth single action dataset.
%We use the common setup of using skeleton data from
%, using all 20 action categories. We
%use 557 videos from the dataset as proposed by \cite{Wang2012}. We use this
%dataset to show how our model performs in a standard database of single
%actions.


\subsection{Detection of Concurrent Actions}
Our second experiment evaluates the performance of our model in a concurrent
action recognition setting. In this scenario, the goal is to predict
the temporal localization of actions that may occur concurrently in a long
video.

We evaluate this task on the Concurrent Actions dataset \cite{Wei2013},
which
provides 61 RGBD videos and pose estimation data annotated with 12
action categories of interest.
We use a similar evaluation setup as proposed by the authors.
We split the dataset into training and testing sets with a 50\%-50\% ratio.
We evaluate performance by measuring precision-recall: a detected action
is declared as a true positive if its temporal overlap with the ground
truth action interval is larger than 60\% of ther union, or if
the detected interval is completely covered by the ground truth annotation.

Our model is tailored at recognizing complex actions that are composed
of atomic components. However, in this scenario, only atomic actions are
provided and no compositions are explicitly defined. We therefore apply
a simple preprocessing step: we cluster training videos into groups
by comparing the occurrence of atomic actions within each video.
The resulting groupings are used as complex actions labels in the training
videos of this dataset.

Note that at inference time, our model outputs a single labeling per video,
which corresponds to the atomic action labeling that maximizes the energy of
our model.
Since there are no thresholds to adjust, our model produces a single
precision-recall measurement as reported in Table \ref{tab:concurrent}.
We observe that our model outperforms the state-of-the-art method in this
dataset at that recall level.


%\paragraph{Concurrent Action dataset} This dataset has 61 videos of variable
%time, some of them are very long comoared to other action datasets. The videos
%has a variable number of actions. The skeleton data and action annotations are
%provided in the dataset.
%We select randomly 50\% of videos for training and the
%rest for testing.

%We want to show using this dataset that the latent formulation achieve good
%recognition performance with respect to the model that uses this dataset. We
%can show also new annotations in this dataset, corresponding to the regions
%that are annotated with the actions.


\begin{table}[tb]
\footnotesize
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Algorithm} & \textbf{Precision} & \textbf{Recall}\\
\hline
%Ours, GEO+VEL, NI, VL-KM-ST &  92.3\% & 0.81\% \\
Our model &  0.92 & 0.81 \\
\hline
Wei et al. \cite{Wei2013} & 0.85 & 0.81 \\
\hline
\end{tabular}
\caption{
\footnotesize
Action detection performances in the Concurrent Actions dataset. }
\label{tab:concurrent}
\vspace{-3mm}
\end{table}
 
\subsection{Recognition of Composable Activities}
In this experiment, we study the performance of our model in terms of
classification of complex and composable actions.
For this purpose, we use the Composable Activities dataset \cite{Lillo2014},
which provides 693 videos of 14 subjects performing 16 composable activities.
Each activity is formed as a spatio-temporal composition of atomic actions.
The dataset provides a total of 26 atomic actions that are shared across
activities.

Table \ref{tab:composable} summarizes our experimental results.
We report activity recognition accuracy for two versions of our
model: one trained with spatial supervision that indicates which
body regions are involved in each action, and another with
no spatial supervision where no body regions are annotated and $V$
is treated as a latent variable.
We observe that both models achieve comparable performance, which
indicates that our weakly supervised model can recover some of the information
that is missing and still perform well at the activity catergorization task.
It is also interesting to note that in spite of using less
supervision at training time, our method outperforms the state-of-the-art
methodologies that are trained with full spatial supervision.
% of the body parts
%and regions that are involved in each atomic actions.


\begin{table}[tb]
\footnotesize
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Algorithm} & \textbf{Accuracy}\\
\hline
Our model, with spatial supervision &  91.8\% \\
Our model, no spatial supervision ($V$ Latent) & 91.1\%\\
%Ours, GEO+TRAJ &  91.1\% \\
%Ours, GEO+TRAJ, NI  & 91.8\% \\
%Ours, GEO+TRAJ, NI, VL-KM-ST   & 91.1\% \\
\hline
%J. Luo et al. \cite{luo2013group} & \textbf{96.7\%} \\
%Y. Zhu et al. \cite{zhu2013fusing} & 94.3\% \\
Lillo \etal \cite{Lillo2014} & 85.7\% \\
%BoW & 74.1\%    \\
%HMM & 78.9\%  \\
Cao et al. \cite{cao2015spatio} & 79.0\% \\
%H-BoW & 82.4\%   \\
%2-lev-HIER & 83.8\%  \\
\hline
\end{tabular}
\caption{
\footnotesize
Human action classification performances in the Composable Activities
dataset.}
\vspace{-4mm}
\label{tab:composable}
\end{table}
 
\subsection{Action Recognition in RGB Videos}
Our experiments so far have evaluated the performance of our model
in the task of human action recognition in RGBD videos.
In this experiment, we explore the use of our model in the problem of human
action recognition in RGB videos. For this purpose, we use the sub-JHMDB
dataset \cite{Jhuang2013}, which focuses on videos depicting 12 actions and
where most of the actor body pose is visible in the image frames.
In our validation, we use the 2D body pose configurations provided by the
authors and compare performance against previous methods that also use them.
We summarize the results of this experiment in Table \ref{tab:subjhmdb},
which shows that our method outperforms alternative action recognition algorithms
in the state-of-the-art.

%\paragraph{sub-JHMDB} In this dataset, we use the annotated joints provided to
%build our geometric descriptor. Only 15 joints per frame are annotated, and the
%coordinates of the joins are in 2D image coordinates.
%We first translate the
%15 joints into 20 joints, and also create a \emph{pseudo} 3D data by adding a
%$z=0$ coordinate to the joints, adding $d$ to the joints of wrists and knees,
%and subtracting $d$ for elbows, to create a 3D skeleton suitable to our model.
%AS RGB videos are available, we compute the TRAJ feature as in Composble
%Acivities Dataset (explain better).

%For a fair
%comparison, we compare our method with works that used the ground truth joints.
%We show in the results the mean accuracy over three splits, provided in the
%dataset.

\begin{table}[tb]
\footnotesize
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Algorithm} & \textbf{Accuracy}\\
\hline
Our model &  77.5\% \\
\hline
Huang et al. \cite{Jhuang2013} & 75.6\% \\
Ch\'eron et al. \cite{Cheron2015} & 72.5\%\\
\hline
\end{tabular}
\caption{\footnotesize
Action classification performances in the sub-JHMDB dataset.}
\label{tab:subjhmdb}
\vspace{-4mm}
\end{table}


\subsection{Spatio-temporal Annotation of Atomic Actions}
In this experiment, we study the ability of our model to spatially annotate
actions.
Table \ref{tab:annotation} summarizes our annotation performance
results.
After training our model, we measure precision-recall for the spatio-temporal
atomic action annotations predicted by our model in the testing videos
(first row).
This is a very challenging task, as
the testing videos are not provided any labels and the model
should predict the temporal extent of each action and the regions of the
body associated with it. We note that our model achieves good performance
in this annotation task.

Furthermore, we also study the capability of the model in annotating
the spatial body region during training. In this case, each video is provided
with the temporal extent of actions and only the spatial anotation needs to
be inferred (second row).
Finally, we note that if we run the trained model over the training videos
without providing any labels (third row), the model is still able to
spatio-temporally annotate atomic actions with performance similar to the
one obtained for the testing set.
%Note that this is due to the fact
%that we are not directly optimizing for atomic action annotation
%We also analyze the performance of the model in discovering the true atomic
%action annotations in the training set.
%our model.


\begin{table}[tb]
\footnotesize
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Videos} & \textbf{Annotation} & \textbf{Precision} & \textbf{Recall}\\
\hline
Testing set & Spatio-temporal   & 0.62 & 0.78 \\
\hline
Training set & Spatial only & 0.86 & 0.90\\
Training set & Spatio-temporal & 0.67 & 0.85 \\
\hline
\end{tabular}
\caption{
\footnotesize
Atomic action annotation performances in the Composable Activities
dataset. The results show that our model is able to recover spatio-temporal
annotations both at training and testing time.}
\label{tab:annotation}
\vspace{-3mm}
\end{table}


\subsection{Effect of Model Components}
In this experiment,
we study the contribution in terms of performance of each component of the
model.
We focus our validation on the Composable Activities dataset for the purpose
of this experiment.

Table \ref{tab:components} summarizes our experimental results with different
versions of our model. We focus on three main components of our model:
the garbage collector for poses (GC), Actionlets, and no spatial supervision
on body regions ($V$ Latent).
Our reported results in Table \ref{tab:components} show that the full version
of our model achieves the best performance, with each component contributing
to the overall success of the method.

%In this dataset is clear tee benefits of all the components of the model: as we
%only have a single action label per video, we use the initialization of videos
%to get a better representation of the actions in the videos.
%As this dataset is
%from videos \emph{on the wild}, the camera view varies from video to video,
%making this dataset specially suitable to our algorithm of multiple classifiers
%per semantic action. Finally, the same as the rest of datasets, including the
%garbage collector math in the model allows to get a more discriminative model
%as it feeds the pose classifiers only with most informative poses.


\begin{table}[tb]
\footnotesize
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Algorithm} & \textbf{Accuracy}\\
\hline
Base Model & 70.6\%\\
Base Model + GC & 72.7\% \\
Base Model + Actionlets & 75.3\%\\
Our full model (Actionlets + GC + $V$ Latent) &  77.5\% \\
\hline
\end{tabular}
\caption{
\footnotesize
Analysis of contribution to recognition performance from
each model component in the sub-JHMDB dataset.}
\label{tab:components}
\vspace{-4mm}
\end{table}

We also analize the contribution of our self-paced learning framework
for intializing and training our model.
As in any discriminative model with latent variables, initialization
is important.
We summarize our results in
Table \ref{tab:initialization} by reporting action
recogntion accuracy obtained by the model under different initialization schemes.
We compare our initialization method to two baselines. First,
we initialize the latent variables $V$ randomly. Second, we initialize $V$ by
performing clustering on region descriptors.
We note that our initialization method help the model to achieve its best
performance.

\begin{table}[tb]
\footnotesize
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Initialization Algorithm} & \textbf{Accuracy}\\
\hline
Random   & 46.3\% \\
Clustering   & 54.8\% \\
Ours   & 91.1\% \\
\hline
\end{tabular}
\caption{
\footnotesize
Results in Composable Activities dataset, using V latent, showing different initializations. }
\label{tab:initialization}
\vspace{-3mm}
\end{table}

\subsection{Qualitative Results}

\begin{figure}[th]
\begin{center}
%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%\includegraphics[width=0.9\linewidth]{./Fig/poselets.pdf}
\scriptsize
 Motion poselet \#4 - most probable action: talking on cellphone\\
 \includegraphics[trim=0 0 0 0.35cm, clip, width=0.49\textwidth]{Fig/poselets1}

 Motion poselet \#7 - most probable action: erasing on board\\
 \includegraphics[trim=0 0 0 0.35cm, clip, width=0.49\textwidth]{Fig/poselets2}

 Motion poselet \#19 - most probable action: waving hand\\
 \includegraphics[trim=0 0 0 0.35cm, clip, width=0.49\textwidth]{Fig/poselets3}
\end{center}
\caption{TBD}
\label{fig:poselets_img}
\end{figure}


\begin{figure}[th]
\begin{center}
\scriptsize
 Motion poselet \#16 - most probable action: tennis swing\\
 \includegraphics[trim=0 0 5.5cm 0.25cm, clip, width=0.49\textwidth]{Fig/poselets4}

 Motion poselet \#34 - most probable action: golf swing\\
 \includegraphics[trim=0 0 5.5cm 0.25cm,clip, width=0.49\textwidth]{Fig/poselets5}

 Motion poselet \#160 - most probable action: bend\\
 \includegraphics[trim=0 0 5.5cm 0.25cm, clip, width=0.49\textwidth]{Fig/poselets6}


\end{center}
\caption{TBD}
\label{fig:poselets_skel}
\end{figure}


\begin{comment}

[GENERAL IDEA]

What we want to show:
\begin{itemize}
\item Show tables of results that can be useful to compare the model.
\item Show how the model is useful for videos of simple and composed actions, since now the level of annotations is similar.
\item Show how the inference produces annotated data (poses, actions, etc). In particular, show in Composable Activities and Concurrent actions how the action compositions are handled by the model without post-processing.
\item Show results in sub-JHMDB,showing how the model detects the action in the videos and also which part of the body performs the action (search for well-behaved videos). It could be interesting to show the annotated data over real RGB videos. 
\item Show examples of poses (like poselets) and sequences of 3 or 5 poses for actions (Actionlets?)
\end{itemize}

\subsection{Figures}
The list of figures should include:
\begin{itemize}
\item A figure showing the recognition and mid-level labels of Composable Activities, using RGB videos
\item Comparison of action annotations, real v/s inferred in training set, showing we can recover (almost) the original annotations.
\item Show a figure similar to Concurrent Actions paper, with a timeline showing the actions in color. We can show that our inference is more stable than proposed in that paper, and it is visually more similar to the ground truth than the other methods.
\item Show a figure for sub-JHMDB dataset, where we can detect temporally and spatially the action without annotations in the training set.
\item Show Composable Activities and sub-JHMDB  the most representative poses and actions.
\end{itemize}


\paragraph{Composable Activities Dataset}
In this dataset we show several results.
(1) Comparing TRAJ descriptor (HOF over trajectory);
(2) Compare the results using latent variables for action assignations to
regions, with different initializations;
(3) Show results of the annotations of the videos in inference.

We must include figures comparing the real annotations
and the inferred annotations for training data, to show we are able to get the
annotations only from data.



%\input{experiments_simple}
% Here we explain the databases, give implementation details, and all the extra info needed to understand the following sections.
\subsection{Recognition of composable activities}
\label{subsec:experiments_summary}
%\input{experiments_composable}

\subsection{Impact of including motion features}
\label{subsec:exp_motionfeats}

\subsection{Impact of latent spatial assignment of actions}
\label{subsec:exp_vlatent}
%\input{experiments_motion}

\subsection{Impact of using multiple classifiers per semantic action}
\label{subsec:exp_multiple}
%\input{experiments_sparse_reg}

\subsection{Impact of handling non-informative poses}
\label{subsec:exp_non_info_handling}
%\input{experiments_non_info_handling}
\end{comment}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%555
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% COMMENTED SECTION FOR CAD120, INCLUDE IT SOMEWHERE IF THERE IS RESULTS IN
% THIS DATASET
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}
\subsection{CAD120 Dataset}
The CAD120 dataset is introduced in \cite{Koppula2012}. It is composed of 124
videos that contain activities in 10 clases performed by 4 actors. Activities
are related to daily living: \emph{making cereal}, \emph{stacking objects}, or
\emph{taking a meal}. Each activity is composed of simpler actions like
\emph{reaching}, \emph{moving}, or \emph{eating}. In this database, human-object
interactions are an important cue to identify the actions, so object
locations and object affordances are provided as annotations. Performance
evaluation is made through leave-one-subject-out cross-validation. Given
that our method does not consider objects, we use only
the data corresponding to 3D joints of the skeletons. As shown in Table
\ref{Table-CAD120},
our method outperforms the results reported in
\cite{Koppula2012} using the same experimental setup. It is clear that using
only 3D joints is not enough to characterize each action or activity in this
dataset. As part of our future work, we expect that adding information related
to objects will further improve accuracy.
%
\begin{table}
\centering
{\small
\begin{tabular}{|c|c|c|}
\hline
\textbf{Algorithm} & \textbf{Average precision} & \textbf{Average recall}\\
\hline
Our method &  32.6\% & 34.58\% \\
\hline
\cite{Koppula2012} &   27.4\% & 31.2\%\\
\cite{Sung2012} &  23.7\%  &  23.7\% \\
\hline
\end{tabular}
}
\caption{Recognition accuracy of our method compared to state-of-the-art methods
using CAD120 dataset.}
\label{Table-CAD120}
\end{table}
\end{comment}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



