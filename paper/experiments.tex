We validate the effectiveness of our proposed model by considering several experimental scenarios.
First, we test the ability of our approach to learn the model classifiers using the latent actions formulation, in databases of composed activities and datasets of simple actions. 
Second, we test the performance of our model when using multiple atomic sequence  classifiers per semantic action, testing in  simple and complex datasets. In every test, the comparison between the activation/deactivation of \emph{garbage colector} will be discussed.

Additionally, we also study the contribution of key components of our model
and their impact in recognition performance.
In particular, we highlight the contribution of the extensions of the
framework with respect to \cite{Lillo2014}.

We tested our improvements in several datasets. In particular we use three datasets of composed actions (Composed Activities dataset \cite{Lillo2014}, Concurrent Actions dataset \cite{Wei2013}, and CAD120 dataset \cite{Koppula2012}), in addition to simple activities dataset like MSR-Action3D \cite{WanLi2010} and sub-JHMDB \cite{Jhuang2013}.

\subsection{Recognition of simple actions}
\label{subsec:exp_setup}

%\input{experiments_simple}
% Here we explain the databases, give implementation details, and all the extra info needed to understand the following sections.

\subsection{Recognition of composable activities}
\label{subsec:experiments_summary}
%\input{experiments_composable}

\subsection{Impact of including motion features}
\label{subsec:exp_vlatent}

\subsection{Impact of latent spatial assignment of actions}
\label{subsec:exp_vlatent}
%\input{experiments_motion}

\subsection{Impact of handling non-informative poses}
\label{subsec:exp_non_info_handling}
%\input{experiments_non_info_handling}

\subsection{Impact of using multiple classifiers per semantic action}
\label{subsec:exp_multiple}
%\input{experiments_sparse_reg}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%555
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% COMMENTED SECTION FOR CAD120, INCLUDE IT SOMEWHERE IF THERE IS RESULTS IN
% THIS DATASET
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}
\subsection{CAD120 Dataset}
The CAD120 dataset is introduced in \cite{Koppula2012}. It is composed of 124
videos that contain activities in 10 clases performed by 4 actors. Activities
are related to daily living: \emph{making cereal}, \emph{stacking objects}, or
\emph{taking a meal}. Each activity is composed of simpler actions like
\emph{reaching}, \emph{moving}, or \emph{eating}. In this database, human-object
interactions are an important cue to identify the actions, so object
locations and object affordances are provided as annotations. Performance
evaluation is made through leave-one-subject-out cross-validation. Given
that our method does not consider objects, we use only
the data corresponding to 3D joints of the skeletons. As shown in Table
\ref{Table-CAD120},
our method outperforms the results reported in
\cite{Koppula2012} using the same experimental setup. It is clear that using
only 3D joints is not enough to characterize each action or activity in this
dataset. As part of our future work, we expect that adding information related
to objects will further improve accuracy.
%
\begin{table}
\centering
{\small
\begin{tabular}{|c|c|c|}
\hline
\textbf{Algorithm} & \textbf{Average precision} & \textbf{Average recall}\\
\hline
Our method &  32.6\% & 34.58\% \\
\hline
\cite{Koppula2012} &   27.4\% & 31.2\%\\
\cite{Sung2012} &  23.7\%  &  23.7\% \\
\hline
\end{tabular}
}
\caption{Recognition accuracy of our method compared to state-of-the-art methods
using CAD120 dataset.}
\label{Table-CAD120}
\end{table}
\end{comment}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



